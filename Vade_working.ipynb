{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v2DOVQNNlkBW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "import itertools\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "def get_data(data=\"MNIST\", batch_size=128):\n",
        "    # Datasets loading\n",
        "    data_dir = f\"./data/{data}/\"\n",
        "    if data == \"MNIST\":\n",
        "        train_dataset = datasets.MNIST(\n",
        "            root=\"./mnist_data/\",\n",
        "            train=True,\n",
        "            transform=transforms.Compose(\n",
        "                [transforms.ToTensor(), transforms.Lambda(torch.flatten)]\n",
        "            ),\n",
        "            download=True,\n",
        "        )\n",
        "        test_dataset = datasets.MNIST(\n",
        "            root=\"./mnist_data/\",\n",
        "            train=False,\n",
        "            transform=transforms.Compose(\n",
        "                [transforms.ToTensor(), transforms.Lambda(torch.flatten)]\n",
        "            ),\n",
        "            download=False,\n",
        "        )\n",
        "    elif data == \"FashionMNIST\":\n",
        "        train_dataset = datasets.FashionMNIST(\n",
        "            root=\"./mnist_data/\",\n",
        "            train=True,\n",
        "            transform=transforms.Compose(\n",
        "                [transforms.ToTensor(), transforms.Lambda(torch.flatten)]\n",
        "            ),\n",
        "            download=True,\n",
        "        )\n",
        "        test_dataset = datasets.FashionMNIST(\n",
        "            root=\"./mnist_data/\",\n",
        "            train=False,\n",
        "            transform=transforms.Compose(\n",
        "                [transforms.ToTensor(), transforms.Lambda(torch.flatten)]\n",
        "            ),\n",
        "            download=False,\n",
        "        )\n",
        "    # Data Loader (Input Pipeline)\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def cluster_acc(Y_pred, Y):\n",
        "    # from sklearn.utils.linear_assignment_ import linear_assignment - is broken\n",
        "    # from scipy.optimize import linear_sum_assignment as linear_assignment - could be a replacement but is a bit different\n",
        "    assert Y_pred.size == Y.size\n",
        "    D = max(Y_pred.max(), Y.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(Y_pred.size):\n",
        "        w[Y_pred[i], Y[i]] += 1\n",
        "    ind = linear_assignment(w.max() - w)\n",
        "    return sum([w[i, j] for i, j in ind]) * 1.0 / Y_pred.size, w\n",
        "\n",
        "\n",
        "def get_hidden_layer(in_dim, out_dim):\n",
        "    return [nn.Linear(in_dim, out_dim), nn.ReLU(True)]\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dims=[512, 512, 2048], stat_dim=10):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.mu_l = nn.Linear(hidden_dims[-1], stat_dim)\n",
        "        self.log_sigma2_l = nn.Linear(hidden_dims[-1], stat_dim)\n",
        "        self.encoder = nn.Sequential(\n",
        "            *get_hidden_layer(input_dim, hidden_dims[0]),\n",
        "            *get_hidden_layer(hidden_dims[0], hidden_dims[1]),\n",
        "            *get_hidden_layer(hidden_dims[1], hidden_dims[2]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e = self.encoder(x)\n",
        "        return self.mu_l(e), self.log_sigma2_l(e)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dims=[512, 512, 2048], stat_dim=10):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            *get_hidden_layer(stat_dim, hidden_dims[-1]),\n",
        "            *get_hidden_layer(hidden_dims[-1], hidden_dims[-2]),\n",
        "            *get_hidden_layer(hidden_dims[-2], hidden_dims[-3]),\n",
        "            nn.Linear(hidden_dims[-3], input_dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x_pro = self.decoder(z)\n",
        "        return x_pro\n",
        "\n",
        "\n",
        "class VaDE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_clusters,\n",
        "        stat_dim,\n",
        "        hidden_dims=[512, 512, 2048],\n",
        "        input_dim=784,\n",
        "        cuda=True,\n",
        "    ):\n",
        "        super(VaDE, self).__init__()\n",
        "        self.n_clusters = n_clusters\n",
        "        self.stat_dim = stat_dim\n",
        "        self.cuda = torch.cuda.is_available() and cuda\n",
        "        self.encoder = Encoder(\n",
        "            input_dim=input_dim, hidden_dims=hidden_dims, stat_dim=stat_dim\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            input_dim=input_dim, hidden_dims=hidden_dims, stat_dim=stat_dim\n",
        "        )\n",
        "        self.pi_ = nn.Parameter(\n",
        "            torch.FloatTensor(\n",
        "                self.n_clusters,\n",
        "            ).fill_(1)\n",
        "            / self.n_clusters,\n",
        "            requires_grad=True,\n",
        "        )\n",
        "        self.mu_c = nn.Parameter(\n",
        "            torch.FloatTensor(self.n_clusters, self.stat_dim).fill_(0),\n",
        "            requires_grad=True,\n",
        "        )\n",
        "        self.log_sigma2_c = nn.Parameter(\n",
        "            torch.FloatTensor(self.n_clusters, self.stat_dim).fill_(0),\n",
        "            requires_grad=True,\n",
        "        )\n",
        "        if self.cuda:\n",
        "            self = self.cuda()\n",
        "            self = nn.DataParallel(self, device_ids=range(4))\n",
        "\n",
        "    def pre_train(self, dataloader, pre_epoch=10):\n",
        "        if not os.path.exists(\"./pretrained_model.pk\"):\n",
        "            Loss_fn = nn.MSELoss()\n",
        "            opti = Adam(\n",
        "                itertools.chain(self.encoder.parameters(), self.decoder.parameters())\n",
        "            )\n",
        "            print(\"Pretraining......\")\n",
        "            epoch_bar = tqdm(range(pre_epoch))\n",
        "            for _ in epoch_bar:\n",
        "                L = 0\n",
        "                for x, y in dataloader:\n",
        "                    if self.cuda:\n",
        "                        x = x.cuda()\n",
        "\n",
        "                    z, _ = self.encoder(x)\n",
        "                    x_ = self.decoder(z)\n",
        "                    loss = Loss_fn(x, x_)\n",
        "\n",
        "                    L += loss.detach().cpu().numpy()\n",
        "\n",
        "                    opti.zero_grad()\n",
        "                    loss.backward()\n",
        "                    opti.step()\n",
        "                epoch_bar.write(\"L2={:.4f}\".format(L / len(dataloader)))\n",
        "            self.encoder.log_sigma2_l.load_state_dict(self.encoder.mu_l.state_dict())\n",
        "            Z = []\n",
        "            Y = []\n",
        "            with torch.no_grad():\n",
        "                for x, y in dataloader:\n",
        "                    if self.cuda:\n",
        "                        x = x.cuda()\n",
        "                    z1, z2 = self.encoder(x)\n",
        "                    assert F.mse_loss(z1, z2) == 0\n",
        "                    Z.append(z1)\n",
        "                    Y.append(y)\n",
        "            Z = torch.cat(Z, 0).detach().cpu().numpy()\n",
        "            Y = torch.cat(Y, 0).detach().numpy()\n",
        "            gmm = GaussianMixture(n_components=self.n_clusters, covariance_type=\"diag\")\n",
        "            pre = gmm.fit_predict(Z)\n",
        "            # print('Acc={:.4f}%'.format(cluster_acc(pre, Y)[0] * 100))\n",
        "            if self.cuda:\n",
        "                self.pi_.data = torch.from_numpy(gmm.weights_).cuda().float()\n",
        "                self.mu_c.data = torch.from_numpy(gmm.means_).cuda().float()\n",
        "                self.log_sigma2_c.data = torch.log(\n",
        "                    torch.from_numpy(gmm.covariances_).cuda().float()\n",
        "                )\n",
        "            else:\n",
        "                self.pi_.data = torch.from_numpy(gmm.weights_).float()\n",
        "                self.mu_c.data = torch.from_numpy(gmm.means_).float()\n",
        "                self.log_sigma2_c.data = torch.log(\n",
        "                    torch.from_numpy(gmm.covariances_).float()\n",
        "                )\n",
        "            torch.save(self.state_dict(), \"./pretrained_model.pk\")\n",
        "        else:\n",
        "            self.load_state_dict(torch.load(\"./pretrained_model.pk\"))\n",
        "\n",
        "    def train(self, dataloader, epochs=100, lr=2e-3, gamma=0.95):\n",
        "        opti = Adam(self.parameters(), lr=lr)\n",
        "        lr_s = StepLR(opti, step_size=10, gamma=gamma)\n",
        "        writer = SummaryWriter(\"./logs\")\n",
        "        epoch_bar = tqdm(range(epochs))\n",
        "        for epoch in epoch_bar:\n",
        "            L = 0\n",
        "            for x, _ in dataloader:\n",
        "                if self.cuda:\n",
        "                    x = x.cuda()\n",
        "                loss = vade.ELBO_Loss(x)\n",
        "                opti.zero_grad()\n",
        "                loss.backward()\n",
        "                opti.step()\n",
        "                L += loss.detach().cpu().numpy()\n",
        "            lr_s.step()\n",
        "            # pre=[]\n",
        "            # tru=[]\n",
        "            # with torch.no_grad():\n",
        "            # for x, y in dataloader:\n",
        "            # if self.cuda:\n",
        "            #    x = x.cuda()\n",
        "            # tru.append(y.numpy())\n",
        "            # pre.append(self.predict(x))\n",
        "            # tru=np.concatenate(tru,0)\n",
        "            # pre=np.concatenate(pre,0)\n",
        "            writer.add_scalar(\"loss\", L / len(DL), epoch)\n",
        "            # writer.add_scalar('acc',cluster_acc(pre,tru)[0]*100,epoch)\n",
        "            writer.add_scalar(\"lr\", lr_s.get_last_lr()[0], epoch)\n",
        "            # epoch_bar.write('Loss={:.4f},ACC={:.4f}%,LR={:.4f}'.format(L/len(DL),cluster_acc(pre,tru)[0]*100,lr_s.get_last_lr()[0]))\n",
        "            epoch_bar.write(\n",
        "                \"Loss={:.4f},LR={:.4f}\".format(L / len(DL), lr_s.get_last_lr()[0])\n",
        "            )\n",
        "\n",
        "    def predict(self, x):\n",
        "        z_mu, z_sigma2_log = self.encoder(x)\n",
        "        z = torch.randn_like(z_mu) * torch.exp(z_sigma2_log / 2) + z_mu\n",
        "        # pi = self.pi_\n",
        "        # log_sigma2_c = self.log_sigma2_c\n",
        "        # mu_c = self.mu_c\n",
        "        y_c = torch.exp(\n",
        "            torch.log(self.pi_.unsqueeze(0))\n",
        "            + self.gaussian_pdfs_log(z, self.mu_c, self.log_sigma2_c)\n",
        "        )\n",
        "        y = y_c.detach().cpu().numpy()\n",
        "        return np.argmax(y, axis=1)\n",
        "\n",
        "    def ELBO_Loss(self, x, L=1, det=1e-10):\n",
        "        L_rec = 0\n",
        "        z_mu, z_sigma2_log = self.encoder(x)\n",
        "        for l in range(L):\n",
        "            z = torch.randn_like(z_mu) * torch.exp(z_sigma2_log / 2) + z_mu\n",
        "            x_pro = self.decoder(z)  # x_pro sometimes has nans\n",
        "            try:\n",
        "                L_rec += F.binary_cross_entropy(x_pro, x)\n",
        "            except:\n",
        "                print(x_pro.min(), x_pro.max())\n",
        "        L_rec = L_rec / L\n",
        "        Loss = L_rec * x.size(1)\n",
        "        # pi=self.pi_\n",
        "        # log_sigma2_c=self.log_sigma2_c\n",
        "        # mu_c=self.mu_c\n",
        "        z = torch.randn_like(z_mu) * torch.exp(z_sigma2_log / 2) + z_mu\n",
        "        y_c = (\n",
        "            torch.exp(\n",
        "                torch.log(self.pi_.unsqueeze(0))\n",
        "                + self.gaussian_pdfs_log(z, self.mu_c, self.log_sigma2_c)\n",
        "            )\n",
        "            + det\n",
        "        )\n",
        "        y_c = y_c / (y_c.sum(1).view(-1, 1))  # batch_size*Clusters\n",
        "        Loss += 0.5 * torch.mean(\n",
        "            torch.sum(\n",
        "                y_c\n",
        "                * torch.sum(\n",
        "                    self.log_sigma2_c.unsqueeze(0)\n",
        "                    + torch.exp(\n",
        "                        z_sigma2_log.unsqueeze(1) - self.log_sigma2_c.unsqueeze(0)\n",
        "                    )\n",
        "                    + (z_mu.unsqueeze(1) - self.mu_c.unsqueeze(0)).pow(2)\n",
        "                    / torch.exp(self.log_sigma2_c.unsqueeze(0)),\n",
        "                    2,\n",
        "                ),\n",
        "                1,\n",
        "            )\n",
        "        )\n",
        "        Loss -= torch.mean(\n",
        "            torch.sum(y_c * torch.log(self.pi_.unsqueeze(0) / (y_c)), 1)\n",
        "        ) + 0.5 * torch.mean(torch.sum(1 + z_sigma2_log, 1))\n",
        "        return Loss\n",
        "\n",
        "    def gaussian_pdfs_log(self, x, mus, log_sigma2s):\n",
        "        G = []\n",
        "        for c in range(self.n_clusters):\n",
        "            G.append(\n",
        "                self.gaussian_pdf_log(\n",
        "                    x, mus[c : c + 1, :], log_sigma2s[c : c + 1, :]\n",
        "                ).view(-1, 1)\n",
        "            )\n",
        "        return torch.cat(G, 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def gaussian_pdf_log(x, mu, log_sigma2):\n",
        "        return -0.5 * (\n",
        "            torch.sum(\n",
        "                np.log(np.pi * 2)\n",
        "                + log_sigma2\n",
        "                + (x - mu).pow(2) / torch.exp(log_sigma2),\n",
        "                1,\n",
        "            )\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.optim import Adam\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "batch_size=800\n",
        "n_clusters=10\n",
        "stat_dim=10\n",
        "\n",
        "DL,_= get_data('MNIST',batch_size)\n",
        "vade=VaDE(n_clusters, stat_dim)\n",
        "vade.pre_train(DL,pre_epoch=2)\n",
        "vade.train(DL, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ARBS6vxlk24",
        "outputId": "b14ebb3e-3511-4863-c35d-42c7267a3a60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 1/3 [00:36<01:13, 36.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss=214.1563,LR=0.0020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 2/3 [01:15<00:37, 37.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss=169.0510,LR=0.0020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [01:52<00:00, 37.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss=153.0145,LR=0.0020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "anY4PUh4DSVn"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}